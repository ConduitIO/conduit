// Code generated by paramgen. DO NOT EDIT.
// Source: github.com/ConduitIO/conduit-commons/tree/main/paramgen

package openai

import (
	"github.com/conduitio/conduit-commons/config"
)

const (
	textgenConfigApiKey              = "api_key"
	textgenConfigBackoffFactor       = "backoff_factor"
	textgenConfigDeveloperMessage    = "developer_message"
	textgenConfigField               = "field"
	textgenConfigFrequencyPenalty    = "frequency_penalty"
	textgenConfigInitialBackoff      = "initial_backoff"
	textgenConfigLogProbs            = "log_probs"
	textgenConfigLogitBias           = "logit_bias.*"
	textgenConfigMaxBackoff          = "max_backoff"
	textgenConfigMaxCompletionTokens = "max_completion_tokens"
	textgenConfigMaxRetries          = "max_retries"
	textgenConfigMaxTokens           = "max_tokens"
	textgenConfigMetadata            = "metadata.*"
	textgenConfigModel               = "model"
	textgenConfigN                   = "n"
	textgenConfigPresencePenalty     = "presence_penalty"
	textgenConfigReasoningEffort     = "reasoning_effort"
	textgenConfigSeed                = "seed"
	textgenConfigStop                = "stop"
	textgenConfigStore               = "store"
	textgenConfigStream              = "stream"
	textgenConfigStrictOutput        = "strict_output"
	textgenConfigTemperature         = "temperature"
	textgenConfigTopLogProbs         = "top_log_probs"
	textgenConfigTopP                = "top_p"
	textgenConfigUser                = "user"
)

func (textgenConfig) Parameters() map[string]config.Parameter {
	return map[string]config.Parameter{
		textgenConfigApiKey: {
			Default:     "",
			Description: "APIKey is the OpenAI API key. Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenConfigBackoffFactor: {
			Default:     "2.0",
			Description: "BackoffFactor is the factor by which the backoff increases. Defaults to 2.0",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenConfigDeveloperMessage: {
			Default:     "",
			Description: "DeveloperMessage is the system message that guides the model's behavior. Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenConfigField: {
			Default:     ".Payload.After",
			Description: "Field is the reference to the field to process. Defaults to \".Payload.After\".",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenConfigFrequencyPenalty: {
			Default:     "",
			Description: "FrequencyPenalty penalizes new tokens based on frequency in text.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenConfigInitialBackoff: {
			Default:     "1000",
			Description: "InitialBackoff is the initial backoff duration in milliseconds. Defaults to 1000ms (1s).",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigLogProbs: {
			Default:     "",
			Description: "LogProbs is whether to return log probabilities of output tokens.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenConfigLogitBias: {
			Default:     "",
			Description: "LogitBias modifies the likelihood of specified tokens appearing.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigMaxBackoff: {
			Default:     "30000",
			Description: "MaxBackoff is the maximum backoff duration in milliseconds. Defaults to 30000ms (30s).",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigMaxCompletionTokens: {
			Default:     "",
			Description: "MaxCompletionTokens is the maximum number of tokens for completion.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigMaxRetries: {
			Default:     "3",
			Description: "MaxRetries is the maximum number of retries for API calls. Defaults to 3.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigMaxTokens: {
			Default:     "",
			Description: "MaxTokens is the maximum number of tokens to generate.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigMetadata: {
			Default:     "",
			Description: "Metadata is additional metadata to include with the request.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenConfigModel: {
			Default:     "",
			Description: "Model is the OpenAI model to use (e.g., gpt-4o-mini). Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenConfigN: {
			Default:     "",
			Description: "N is the number of completions to generate.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigPresencePenalty: {
			Default:     "",
			Description: "PresencePenalty penalizes new tokens based on presence in text.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenConfigReasoningEffort: {
			Default:     "",
			Description: "ReasoningEffort controls the amount of reasoning in the response.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenConfigSeed: {
			Default:     "",
			Description: "Seed is the seed for deterministic results.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigStop: {
			Default:     "",
			Description: "Stop are sequences where the API will stop generating.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenConfigStore: {
			Default:     "",
			Description: "Store is whether to store the conversation in OpenAI.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenConfigStream: {
			Default:     "",
			Description: "Stream is whether to stream the results or not. Not used for now.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenConfigStrictOutput: {
			Default:     "false",
			Description: "StrictOutput enforces strict output format. Defaults to false.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenConfigTemperature: {
			Default:     "",
			Description: "Temperature controls randomness (0-2, lower is more deterministic).",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenConfigTopLogProbs: {
			Default:     "",
			Description: "TopLogProbs is the number of most likely tokens to return probabilities for.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenConfigTopP: {
			Default:     "",
			Description: "TopP controls diversity via nucleus sampling.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenConfigUser: {
			Default:     "",
			Description: "User is the user identifier for OpenAI API.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
	}
}
