// Code generated by paramgen. DO NOT EDIT.
// Source: github.com/ConduitIO/conduit-commons/tree/main/paramgen

package textgen

import (
	"github.com/conduitio/conduit-commons/config"
)

const (
	textgenProcessorConfigApiKey              = "api_key"
	textgenProcessorConfigBackoffFactor       = "backoff_factor"
	textgenProcessorConfigDeveloperMessage    = "developer_message"
	textgenProcessorConfigField               = "field"
	textgenProcessorConfigFrequencyPenalty    = "frequency_penalty"
	textgenProcessorConfigInitialBackoff      = "initial_backoff"
	textgenProcessorConfigLogProbs            = "log_probs"
	textgenProcessorConfigLogitBias           = "logit_bias.*"
	textgenProcessorConfigMaxBackoff          = "max_backoff"
	textgenProcessorConfigMaxCompletionTokens = "max_completion_tokens"
	textgenProcessorConfigMaxRetries          = "max_retries"
	textgenProcessorConfigMaxTokens           = "max_tokens"
	textgenProcessorConfigMetadata            = "metadata.*"
	textgenProcessorConfigModel               = "model"
	textgenProcessorConfigN                   = "n"
	textgenProcessorConfigPresencePenalty     = "presence_penalty"
	textgenProcessorConfigReasoningEffort     = "reasoning_effort"
	textgenProcessorConfigSeed                = "seed"
	textgenProcessorConfigStop                = "stop"
	textgenProcessorConfigStore               = "store"
	textgenProcessorConfigStream              = "stream"
	textgenProcessorConfigStrictOutput        = "strict_output"
	textgenProcessorConfigTemperature         = "temperature"
	textgenProcessorConfigTopLogProbs         = "top_log_probs"
	textgenProcessorConfigTopP                = "top_p"
	textgenProcessorConfigUser                = "user"
)

func (textgenProcessorConfig) Parameters() map[string]config.Parameter {
	return map[string]config.Parameter{
		textgenProcessorConfigApiKey: {
			Default:     "",
			Description: "APIKey is the OpenAI API key. Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenProcessorConfigBackoffFactor: {
			Default:     "2.0",
			Description: "BackoffFactor is the factor by which the backoff increases. Defaults to 2.0",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigDeveloperMessage: {
			Default:     "",
			Description: "DeveloperMessage is the system message that guides the model's behavior. Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenProcessorConfigField: {
			Default:     ".Payload.After",
			Description: "Field is the reference to the field to process. Defaults to \".Payload.After\".",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigFrequencyPenalty: {
			Default:     "",
			Description: "FrequencyPenalty penalizes new tokens based on frequency in text.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigInitialBackoff: {
			Default:     "1000",
			Description: "InitialBackoff is the initial backoff duration in milliseconds. Defaults to 1000ms (1s).",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigLogProbs: {
			Default:     "",
			Description: "LogProbs is whether to return log probabilities of output tokens.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigLogitBias: {
			Default:     "",
			Description: "LogitBias modifies the likelihood of specified tokens appearing.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigMaxBackoff: {
			Default:     "30000",
			Description: "MaxBackoff is the maximum backoff duration in milliseconds. Defaults to 30000ms (30s).",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigMaxCompletionTokens: {
			Default:     "",
			Description: "MaxCompletionTokens is the maximum number of tokens for completion.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigMaxRetries: {
			Default:     "3",
			Description: "MaxRetries is the maximum number of retries for API calls. Defaults to 3.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigMaxTokens: {
			Default:     "",
			Description: "MaxTokens is the maximum number of tokens to generate.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigMetadata: {
			Default:     "",
			Description: "Metadata is additional metadata to include with the request.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigModel: {
			Default:     "",
			Description: "Model is the OpenAI model to use (e.g., gpt-4o-mini). Required.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{
				config.ValidationRequired{},
			},
		},
		textgenProcessorConfigN: {
			Default:     "",
			Description: "N is the number of completions to generate.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigPresencePenalty: {
			Default:     "",
			Description: "PresencePenalty penalizes new tokens based on presence in text.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigReasoningEffort: {
			Default:     "",
			Description: "ReasoningEffort controls the amount of reasoning in the response.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigSeed: {
			Default:     "",
			Description: "Seed is the seed for deterministic results.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigStop: {
			Default:     "",
			Description: "Stop are sequences where the API will stop generating.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigStore: {
			Default:     "",
			Description: "Store is whether to store the conversation in OpenAI.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigStream: {
			Default:     "",
			Description: "Stream is whether to stream the results or not. Not used for now.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigStrictOutput: {
			Default:     "false",
			Description: "StrictOutput enforces strict output format. Defaults to false.",
			Type:        config.ParameterTypeBool,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigTemperature: {
			Default:     "",
			Description: "Temperature controls randomness (0-2, lower is more deterministic).",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigTopLogProbs: {
			Default:     "",
			Description: "TopLogProbs is the number of most likely tokens to return probabilities for.",
			Type:        config.ParameterTypeInt,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigTopP: {
			Default:     "",
			Description: "TopP controls diversity via nucleus sampling.",
			Type:        config.ParameterTypeFloat,
			Validations: []config.Validation{},
		},
		textgenProcessorConfigUser: {
			Default:     "",
			Description: "User is the user identifier for OpenAI API.",
			Type:        config.ParameterTypeString,
			Validations: []config.Validation{},
		},
	}
}
