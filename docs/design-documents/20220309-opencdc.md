# OpenCDC

## Introduction

### Goals

The main goal is to ensure compatibility between any combination of source/destination connectors.

Define a format that fulfills the following criteria:
- A record can be associated to a schema that describes the payload or key
- The payload/key schema can be included in the record or pointing to a schema stored in a schema registry
- Be able to represent an inserted, updated and deleted record
- Be able to represent a record created in a snapshot
- Support for splitting big records into multiple parts
- Define standard metadata fields

Besides the format itself, we want to roughly define how Conduit will work with the record, specifically:
- How transforms change the associated schema?
- How can a schema be attached to a raw record?
- How can a schema be extracted to a schema registry?

### Questions

- **Should we have a separate field announcing the record was created as part of a snapshot?**
  - No, this information will be stored in the action (insert, update, delete, snapshot).
- **Should the record support sending a schema only (e.g. we transmit a CREATE TABLE statement)? This might be useful for setting up the target structure even if the source is still empty.**
  - No, this kind of data would only make sense in straight replication between two systems without any transforms. Connectors should rather detect differences in schemas lazily based on each record (more info: https://github.com/ConduitIO/conduit/pull/326#discussion_r835596003).
- **Should the specification of a plugin include the info about what data a plugin can work with? More specifically, the plugin could announce if it can handle raw data or not (e.g. postgres right now needs structured data). If we had this info we could let the user know in advance that a pipeline config is invalid or that a schema needs to be added to records so that they can be converted to structured data.**
  - No, this is out of scope for now.
- **Should the record include a diff in case of an update (old values vs. new values)? We need at least the old key values in case the key value was updated to correctly identify the record.**
  - Yes, before / after values should be supported.

### Background

Here are a couple of scenarios that we want to support. Records in these examples are formatted in the format defined in implementation option 1.

#### Scenario 1: Connector ships raw data without schema

In this scenario we have a pipeline that reads from a schemaless source (for example Kafka). The key and payload that is read is raw, the connector isn't aware of what the structure is so it creates an OpenCDC record with the raw key, raw payload and some metadata and sends it to Conduit for further processing.

The record that Conduit receives could look something like this (note that the record is an internal Go struct, the JSON representation is strictly for illustration purposes):

```json
{
  "metadata": {
    // openCDC standard metadata with opencdc prefix
    "opencdc.readAt": 1647216000123456789, // unix nano timestamp when the message was read by the connector
    "opencdc.createdAt": 1647023456987654321, // unix nano timestamp when the message was written to Kafka
    // plugin metadata, prefix is the plugin name
    "kafka.topic": "my-topic", // kafka topic of message
    "kafka.offset": 123, // kafka offset of message
  },
  "position": "my-topic:123", // internal position
  "operation": "insert", // all records coming from kafka have the operation insert
  "before": null, // insert operation has no before state
  "after": {
    "key": {
      "schema": null, // no schema
      "data": "/.../" // raw kafka message key
    },
    "payload": {
      "schema": null, // no schema
      "data": "/.../" // raw kafka message payload
    }
  }
}
```

#### Scenario 2: Connector ships structured data without schema

A connector could read from a source that supplies structured data, but has no idea about the schema of that structured data (for example a connector exposing a webhook accepting any JSON request). The connector supplies an OpenCDC record with a structured payload and/or key and sends it to Conduit for further processing. Note that the connector could infer the schema from the structured payload, but it does not need to do that since Conduit could do the same thing.

Example record that Conduit receives in this case:

```json
{
  "metadata": {
    // openCDC standard metadata with opencdc prefix
    "opencdc.readAt": 1647216000123456789, // unix nano timestamp when the record was read by the connector
    // notice there is no "createdAt", all fields in metadata are optional
    // plugin metadata, prefix is the plugin name
    "webhook.port": "8080", // port of webhook
    "webhook.endpoint": "/demo", // endpoint where on which the request was received
  },
  "position": "/demo:5", // internal position
  "operation": "insert",
  "before": null,
  "after": {
    "key": null // no key
    "payload": {
      "schema": null, // no schema
      "data": {
        "username": "foo",
        "role": "user"
      }
    }
  }
}
```

#### Scenario 3: Connector ships raw data with schema

This scenario assumes the connector provides raw data but also provides a schema for parsing the raw data and using it as structured data (for example a connector dealing with protobuf data). The reason behind this might be that the raw data can be compressed better and would improve the performance of the pipeline.


Example record that Conduit receives in this case:

```json
{
  "metadata": {
    // openCDC standard metadata with opencdc prefix
    "opencdc.readAt": 1647216000123456789, // unix nano timestamp when the record was read by the connector
  },
  "position": "protobuf-record:5", // internal position
  "operation": "insert",
  "before": null,
  "after": {
    "key": null // no key
    "payload": {
      "schema": {
        "type": "protobuf",
        "value": "/.../" // protobuf schema
      },
      "data": /.../ // raw protobuf message bytes
    }
  }
}
```

#### Scenario 4: Connector ships structured data with schema

In this scenario we have a pipeline that reads from a source that contains structured data (for example Postgres). The connector is aware of the structure so it extracts the schema and structures the data into an OpenCDC record that gets sent to Conduit for further processing.

Example record that Conduit receives in this case:

```json
{
  "metadata": {
    // openCDC standard metadata with opencdc prefix
    "opencdc.readAt": 1647216000123456789, // unix nano timestamp when the message was read by the connector
    "opencdc.createdAt": 1647216000123456789, // unix nano timestamp when the message was written to Kafka
    // plugin metadata, prefix is the plugin name
    "postgres.table": "mytable", // source table where the record originated
    "postgres.lsn": "16/B374D848", // LSN of when change occurred
  },
  "position": "16/B374D848", // internal position
  "operation": "update",
  "before": {
    "key": {
      "schema": {
        // TODO schema format to be defined
      },
      "data": {
        "id": 123
      }
    },
    "payload": {
      "schema": {
        // TODO schema format to be defined
      },
      "data": {
        "username": "foo",
        "role": "admin"
      }
    }
  },
  "after": {
    "key": {
      "schema": {
        // TODO schema format to be defined
      },
      "data": {
        "id": 123
      }
    },
    "payload": {
      "schema": {
        // TODO schema format to be defined
      },
      "data": {
        "username": "foo",
        "role": "user"
      }
    }
  }
}
```

#### Parsing raw payload to OpenCDC record

In scenario 1 the connector ships raw key and payload data to Conduit without knowing the format of the message. It might be the case that the payload itself is an OpenCDC record. For that case Conduit should supply a "parser transform" that can parse the raw payload as an OpenCDC record and do one of two options:

- Use specific data from the parsed record to enrich the current OpenCDC record (e.g. extract payload and replace the current payload). The enriched record would be sent further down the pipeline.
- Replace the current OpenCDC record entirely by sending the parsed record further down the pipeline and discarding the original record.

Note that this transform could only operate on the key or payload data at a time. If both the key and payload contain an OpenCDC record, then two chained transforms would need to be used to parse both.

Note also that we could provide such transforms not only for OpenCDC but also for other formats (e.g. Debezium records).

## Implementation options

The format that we choose and define here wouldn't be constrained by a specific representation like JSON or protobuf. We would need a protobuf definition of the record to be able to move it from the plugin to Conduit through gRPC, but the record could just as well be formatted as JSON or another format.

### Option 1

We consider our needs, define OpenCDC from scratch and change our record to match OpenCDC.

Note that since this is the recommended option we go into more details about how the record should change if we go ahead with this option.

#### Pros

- We have full flexibility in defining a record that matches our needs.
- When people work with our internal Record it makes more sense because it is actually the same as an OpenCDC record.
- If our record is marshalled into JSON (or avro or proto) it's OpenCDC compatible by default.

#### Cons

- We define a new format that nobody is using but us. https://xkcd.com/927/
- Breaking change for connectors, the record will change.
- Debezium compatibility has to be achieved through transforms.

#### Changes to record

We propose the following changes to the `Record` type:

```diff
type Record struct {
	// Position uniquely represents the record.
	Position Position
	// Metadata contains additional information regarding the record.
	Metadata map[string]string
+	// Operation defines if this record contains data related to an insert,
+	// update, delete or snapshot.
+	Operation Operation
-	// CreatedAt represents the time when the change occurred in the source
-	// system. If that's impossible to find out, then it should be the time the
-	// change was detected by the connector.
-	CreatedAt time.Time
-	// Key represents a value that should be the same for records originating
-	// from the same source entry (e.g. same DB row). In a destination Key will
-	// never be null.
-	Key Data
-	// Payload holds the actual information that the record is transmitting. In
-	// a destination Payload will never be null.
-	Payload Data
+	// Before holds the key and payload data that was valid before the change.
+	// This field is only populated if the operation is update or delete.
+	Before struct {
+		// Key contains the data and schema of the key before the change.
+		Key DataWithSchema
+		// Payload contains the data and schema of the payload before the change.
+		Payload DataWithSchema
+	}
+	// After holds the key and payload data that is valid after the change.
+	// This field is only populated if the operation is insert, update or snapshot.
+	After struct {
+		// Key contains the data and schema of the key after the change.
+		Key DataWithSchema
+		// Payload contains the data and schema of the payload after the change.
+		Payload DataWithSchema
+	}
}

+ // Operation defines what operation triggered the creation of the record.
+ type Operation string
+
+ const (
+ 	OperationInsert Operation = "insert"
+ 	OperationUpdate Operation = "update"
+ 	OperationDelete Operation = "delete"
+ 	OperationSnapshot Operation = "snapshot"
+ )

+ // DataWithSchema holds some data and a schema describing the data.
+ type DataWithSchema interface {
+ 	// Schema returns the schema describing Data or nil if none is set.
+ 	Schema() Schema
+ 	// Data returns raw or structured data or nil if none is set.
+ 	Data() Data
+ }
```

Notable changes:

- `Operation` is added to denote what operation triggered the creation of this record.
- `CreatedAt` was moved to metadata (see [standard metadata fields](#standard-metadata-fields)).
- `Key` and `Payload` have been moved under `Before` and `After` to have the ability to represent an updated record.
- `DataWithSchema` was introduced to create a way of attaching a schema to some data.

#### Standard metadata fields

We propose to define a list of standard metadata fields, like:

- `opencdc.createdAt` - This was previously part of the record but would be moved to metadata as it is an optional field not critical to the operation of Conduit. It represents the time when the change occurred in the source system. If that's impossible to find out, then it should be empty.
- `opencdc.readAt` - It represents the time the change was detected by the connector.
- `opencdc.chunk` - Signifies that this record is only a chunk of a bigger record (the behavior and actual content of this field is yet to be designed, that's a whole separate design document).

Conduit could define its own standard metadata fields that it populates automatically, like:

- `conduit.version` - Version of Conduit that processed this record.
- `conduit.plugin.name` - Name of the Conduit plugin that produced this record.
- `conduit.plugin.version` - Version of the Conduit plugin that produced this record.

More standard fields can be added. The SDK would provide an easy way of retrieving standard OpenCDC and Conduit metadata fields.

#### Schema format

It should be possible to attach a schema to a record key/payload for at least these formats:

- JSON (JSONSchema)
- Protobuf
- Avro

Additionally we might want to support an OpenCDC schema format that describes structured data (See [scenario 4](#scenario-4-connector-ships-structured-data-with-schema)).

### Option 2

OpenCDC gets defined the same as the Debezium record, we change our record to match OpenCDC.

#### Pros

- We are Debezium compatible out of the box.
- When people work with our internal Record it makes more sense because it is actually the same as an OpenCDC record.
- If our record is marshalled into JSON (or avro or proto) it's OpenCDC compatible by default.

#### Cons

- Breaking change for connectors, the record will change.
- We are tied to the Debezium format (less flexibility).
- The Debezium format contains a schema describing the record itself (i.e. fields that are part of the format itself), it is redundant.
- The Debezium format is tailored to databases, we could be dealing with any source that is not a database (schemaless raw data, schemaless structured data etc.).

### Option 3

We define OpenCDC by starting with our current record structure and extending it with additional fields to keep it backwards compatible.

#### Pros

- Backwards compatibility.
- We are somewhat flexible in defining something that works for us.

#### Cons

- We are constrained in the design with the current record.
- We define a new format that nobody is using but us.
- Debezium compatibility has to be achieved through transforms.

### Recommendation

Option 1 seems like the way to go and since Conduit is still a relatively new product now is the time to do such a change. Changing the format at a later stage when we have more connectors and users would be much more painful.
