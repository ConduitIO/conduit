# OpenCDC

## Goals

Define a format that fulfills the following criteria:
- A record can be associated to a schema that describes it
  - Schema needs to be included in the record or pointing to a schema stored in a schema registry
- Transformations don't need to know about the schema
  - Conduit should provide functions that change the record and adjust the schema behind the scenes
- Be able to represent an inserted, updated and deleted record
- Support for splitting big records into multiple parts (is this needed?)
- Define standard metadata fields
- Be able to provide a schema alongside the record
- Be able to attach a schema to a record that has no schema (e.g. if the connector supplied a raw record)
- Be able to extract the schema from the record and use a schema registry instead
- Be able to infer a schema from a structured payload

## Non-Goals:

- Conduit shouldn't guard against invalid schema changes

## Questions

- Should we have a separate field announcing the record was created as part of a snapshot? Should Conduit be aware there is a snapshot mode and a CDC mode?
- Should the specification of a plugin include the info about what data a plugin can work with? More specifically, the plugin could announce if it can handle raw data or not (e.g. postgres right now needs structured data). If we had this info we could let the user know in advance that a pipeline config is invalid or that a schema needs to be added to records so that they can be converted to structured data.
- Should the record include a diff in case of an update (old values vs. new values)? We need at least the old key values in case the key value was updated to corrently identify the record.
- Should the record carry the version if the OpenCDC format?
- How closely do we want OpenCDC to follow the Debezium format?
  - The `before` field doesn't seem useful, also it's questionable if we can populate it for any connector other than DB connectors connected to logical replication.
  - The Debezium format outputs two JSONs, one for the key and one for the payload. What if we wanted to output a single JSON that describes the key AND the value at the same time?

## Notes

- Possible data / schema combinations:
  - Raw data without schema
    - Example: binary data
    - In this case some transformations are simply not possible, because data is raw
    - We should allow Conduit to attach a schema to the raw data, this **should not** be a plugin functionality but rather in the core
  - Raw data with schema
    - Example: protobuf raw data with proto schema
    - Conduit should provide a transformation that transforms the data into structured data - this should only happen on demand (or lazily behind the scenes), because parsing might be a costly operation and might not be needed
  - Structured data without schema
    - Example: JSON converted to a map
    - A schema could theoretically be inferred
      - The problem are schema definitions that require field ordering (e.g. protobuf) - in that case an object that is missing a field would generate a different schema than an object with that field (we can try to work around that by hashing field names to field order numbers but can't guarantee there won't be conflicts)
      - Also enums can't be inferred based on a single object
    - Conduit should again allow attaching a schema to the structured data
  - Structured data with schema
    - Example: map + JsonSchema
    - Does it make sense to allow the schema to be defined in JsonSchema / Avro / Proto if the data is already passed to Conduit in the parsed form?

## Scenarios

### Debezium -> Kafka -> Conduit

Debezium connector writes to Kafka, Conduit consumes messages from Kafka with the Kafka plugin.

Expected behavior:
- Kafka plugin consumes data from Kafka and is unaware of the OpenCDC format, it produces raw records, looking like this (note that the raw record is an internal Go struct, the JSON representation is strictly for illustration purposes):
  ```json
   {
     "position": "foo:123,bar:234", // internal position
     "metadata": {
       // plugin metadata, prefix is the plugin name
       "kafka.topic": "foo", // kafka topic of message
       "kafka.offset": 123,   // kafka offset of message
       // conduit metadata, prefix is conduit
       "conduit.plugin.name": "kafka", // name of the conduit plugin
       "conduit.plugin.version": "v0.1.0", // version of the conduit plugin
       "conduit.connector.name": "my-kafka-source", // name of the conduit connector
     },
     "operation": "insert", // all records coming from kafka have the operation insert
     "readAt": "2022-03-09 01:23:45.678901Z", // the time when the message was read by the connector
     "createdAt": "2022-03-08 12:34:56.123456Z", // the time when the message was written to Kafka
     "before": nil, // insert operation has no before state
     "after": {
       "schema": nil, // no schema supplied
       "key": "[raw kafka key as string, 1:1 output of Debezium]",
       "payload": "[raw kafka payload as string, 1:1 output of Debezium]"         
     }
   }
  ```
- Conduit provides a transform that parses the record key and payload into the internal record if it contains an OpenCDC record (debatable if this should be configured by default or opt-in):
  ```diff
   {
     "position": "foo:123,bar:234", // internal position
     "metadata": {
       // plugin metadata, prefix is the plugin name
       "kafka.topic": "foo", // kafka topic of message
       "kafka.offset": 123,   // kafka offset of message
       // conduit metadata, prefix is conduit
       "conduit.plugin.name": "kafka", // name of the conduit plugin
       "conduit.plugin.version": "v0.1.0", // version of the conduit plugin
       "conduit.connector.name": "my-kafka-source", // name of the conduit connector
  +    // OpenCDC metadata extracted from message, prefix is opencdc
  +    "opencdc.source.version": "1.8.1.Final",
  +    "opencdc.source.connector": "postgresql",
  +    "opencdc.source.name": "PostgreSQL_server",
  +    "opencdc.source.ts_ms": 1559033904863,
  +    "opencdc.source.snapshot": false,
  +    "opencdc.source.db": "postgres",
  +    "opencdc.source.schema": "public",
  +    "opencdc.source.table": "customers",
  +    "opencdc.source.txId": 556,
  +    "opencdc.source.lsn": 24023128,
  +    "opencdc.source.xmin": null
     },
  -  "operation": "insert", // all records coming from kafka have the operation insert
  +  "operation": "delete", // operation parsed from OpenCDC message
     "readAt": "2022-03-09 01:23:45.678901Z", // the time when the message was read by the connector
  -  "createdAt": "2022-03-08 12:34:56.123456Z", // the time when the message was written to Kafka
  +  "createdAt": "2022-02-02 23:45:01.123456Z", // the time extracted out of the OpenCDC message (field `source.ts_ms`, if it's empty use `ts_ms`)
  -  "before": nil, // insert operation has no before state
  +  "before": { // before state extracted from OpenCDC record
  +    "schema": { // schema taken from OpenCDC message
  +      "key": {
  +        ... (?)
  +      },
  +      "payload": {
  +        ... (?)
  +      }
  +    },
  +    "key": {
  +      "keyField1": "foo",
  +      "keyField2": 321, // notice the key was updated
  +    },
  +    "payload": nil // payload in before is optional and probably empty most of the time
  +  }
  -  "after": {
  -    "schema": nil, // no schema supplied
  -    "key": "[raw kafka key as string, 1:1 output of Debezium]",
  -    "payload": "[raw kafka payload as string, 1:1 output of Debezium]"         
  -  }
  +  "after": { // after state extracted from OpenCDC record
  +    "schema": { // schema taken from OpenCDC message
  +      "key": {
  +        ... (?)
  +      },
  +      "payload": {
  +        ... (?)
  +      }
  +    },
  +    "key": {
  +      "keyField1": "foo",
  +      "keyField2": 123, // notice the key was updated
  +    },
  +    "payload": {
  +      // payload parsed into a structured payload
  +      // note that debezium records contain the key fields in the payload as well
  +      "keyField1": "foo",
  +      "keyField2": 123,
  +      "payloadField1": "bar",
  +      "payloadField2": "baz"
  +    }
  +  }
   }
  ```

## Changes to record

We propose the following changes to the `Record` type:
```diff
type Record struct {
	// Position uniquely represents the record.
	Position Position
	// Metadata contains additional information regarding the record.
	Metadata map[string]string
+	// Operation defines if this record is an insert, update or delete.
+	Operation Operation
+	// IsSnapshot defines if this record comes from a snapshot.
+	IsSnapshot bool
+	// ReadAt represents the time at which the Conduit connector read the record.
+	ReadAt time.Time
	// CreatedAt represents the time when the change occurred in the source
	// system. If that's impossible to find out, then it should be the time the
	// change was detected by the connector.
	CreatedAt time.Time
-	// Key represents a value that should be the same for records originating
-	// from the same source entry (e.g. same DB row). In a destination Key will
-	// never be null.
-	Key Data
-	// Payload holds the actual information that the record is transmitting. In
-	// a destination Payload will never be null.
-	Payload Data
+	Before struct {
+		// Key represents a value that should be the same for records originating
+		// from the same source entry (e.g. same DB row). In a destination Key will
+		// never be null.
+		Key Data
+		// Payload holds the actual information that the record is transmitting. In
+		// a destination Payload will never be null.
+		Payload Data
+	}
+	After struct {
+		// Key represents a value that should be the same for records originating
+		// from the same source entry (e.g. same DB row). In a destination Key will
+		// never be null.
+		Key Data
+		// Payload holds the actual information that the record is transmitting. In
+		// a destination Payload will never be null.
+		Payload Data
+	}
}

+ type Operation string
+
+ const (
+ 	OperationInsert Operation = "insert"
+ 	OperationUpdate Operation = "update"
+ 	OperationDelete Operation = "delete"
+ )
```

## Design options

### Proposal 1

- OpenCDC is the same as the Debezium record, we provide a transform that parses an OpenCDC record into our internal record.

Pros:
- Backwards compatible code.

Cons:
- We are promoting OpenCDC but using something else internally.
- If a plugin wants to output an OpenCDC record it needs to be changed, since it needs to be transformed.

### Proposal 2

- OpenCDC is the same as the Debezium record, we change our internal Record to have the same fields as the OpenCDC record.

Pros:
- When people work with our internal Record it makes more sense because it is actually the same as an OpenCDC record.
- If our record is marshalled into JSON (or another format) it's OpenCDC compatible by default.

Cons:
- This creates a breaking change in the plugin protocol. Plugins will need to be adjusted.
- We might not need all fields defined in the OpenCDC record.

### Proposal 3

- We extend our Record with additional fields and define the OpenCDC record to match whatever we define.

Pros:
- We can make it backwards compatible because we would be building upon our existing Record.
- We have flexibility to define something that works for us.

Cons:
- We define a new format that nobody is using but us.
